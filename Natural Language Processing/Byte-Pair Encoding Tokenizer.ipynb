{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9db842-f8da-42fd-8ba6-46ea1394395c",
   "metadata": {},
   "source": [
    "# **Byte-Pair Encoding Tokenizer**\n",
    "\n",
    "A tokenizer is simply a function that takes in a sentence and outputs a list of tokens, which are usually subwords. \n",
    "This is done by choosing the tokens that are, in some sense, frequent and important in a given corpus. The number of possible tokens is contained in a vocabulary set.\n",
    "\n",
    "Example:\n",
    "$$\n",
    "    \\text{the cat is sleeping.} \\rightarrow \\text{th|e\\;|ca|t| |is |s|l|ee|p|ing|.| }\n",
    "$$\n",
    "\n",
    "A Byte-Pair Encoding Tokenizer is built to recursively add new tokens to the vocabulary until a fixed side has been reached.\n",
    "\n",
    "This is done by:\n",
    "- Breaking down the training corpus into single characters, and add create a set with them as the initial vocabulary\n",
    "- While k new tokens have not been added to this vocabulary:\n",
    "    - find the most frequent pair viable of tokens (no crossing white-spaces)\n",
    "    - add this pair to the vocabulary\n",
    "    - merge every instance of the pair on the corpus\n",
    "    - add the merging (e.g. ('a', 'b') -> 'ab') to the list of merging operations\n",
    "\n",
    "When tokenizing a new sentence, the merge operations are done in order on a given sentence.\n",
    "\n",
    "One thing that is important to keep in mind is that tokenization usually return the index of the token in the list of possible tokens (i.e. vocabulary). These indexes are then usually provided to an Embedding Table, which is optimized using Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9615fc3-b06e-4629-8503-6608d26eaab9",
   "metadata": {},
   "source": [
    "## 1. Import the Corpus\n",
    "\n",
    "We first read the Dracula book into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a48bbe6-7ecf-47af-8430-95aa17a47286",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dracula.txt', 'r') as f:\n",
    "    book = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4573c-6687-458f-b288-e1f8b3678ee7",
   "metadata": {},
   "source": [
    "## 2. Defining BPE Tokenizer\n",
    "\n",
    "The class will contain a .fit() function that will be used to stack the merge operations using the frequencies of a corpus, and then a .tokenize() function that will apply the merge operations in order to tokenize a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ff4cfb-3fcf-401a-8e97-d6882f39420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.merge_ops = []\n",
    "        self.vocab = []\n",
    "        self.k = k\n",
    "\n",
    "    # This function uses a corpus to build a list of merge operations\n",
    "    def fit(self, corpus):\n",
    "\n",
    "        self._reset()\n",
    "        \n",
    "        # Getting initial vocabulary (single chars)\n",
    "        vocab  = list(set(corpus))\n",
    "        len_init_vocab = len(vocab)\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Breaking corpus into single chars\n",
    "        corpus_tk = list(corpus)\n",
    "\n",
    "        # While k new words have not been added to vocab\n",
    "        while (len(vocab) < len_init_vocab + self.k):\n",
    "\n",
    "            # Get most common pair\n",
    "            pair = self._most_common_pair(corpus_tk)\n",
    "\n",
    "            # If the corpus is too small, we might run out of pairs before k subwords are added\n",
    "            # If that happens, break\n",
    "            if len(pair)==0:\n",
    "                break\n",
    "\n",
    "            # Apply merge operation\n",
    "            corpus_tk = self._merge(corpus_tk, pair)\n",
    "        \n",
    "            # Store pair\n",
    "            self.merge_ops.append(pair)\n",
    "\n",
    "            # Increase vocabulary\n",
    "            self.vocab.append(''.join(pair))\n",
    "\n",
    "    # This function tokenizes a sentence with a trained tokenizer\n",
    "    # If return_idx=True, it returns the indexes of the tokens on the vocabulary instead of the actual strings\n",
    "    def tokenize(self, sentence, return_idx=False):\n",
    "        \n",
    "        # Break down the sentence into single chars\n",
    "        tokens = list(sentence)\n",
    "\n",
    "        # Apply merge operations in the same order they were added\n",
    "        for op in self.merge_ops:\n",
    "            tokens = self._merge(tokens, op)\n",
    "\n",
    "        if return_idx:\n",
    "            return [self.vocab.index(token) for token in tokens]\n",
    "        else:\n",
    "            return tokens\n",
    "\n",
    "    def _merge(self, corpus_tk, pair):\n",
    "        new_corpus_tk = []\n",
    "        i=0\n",
    "        while i < len(corpus_tk):\n",
    "            w1 = corpus_tk[i]\n",
    "            w2 = corpus_tk[i+1] if i<len(corpus_tk)-1 else None\n",
    "            if (w1,w2) == pair:\n",
    "                new_corpus_tk.append(''.join(pair))\n",
    "                i += 2\n",
    "            else:\n",
    "                new_corpus_tk.append(w1)\n",
    "                i += 1\n",
    "        return new_corpus_tk\n",
    "                \n",
    "    def _most_common_pair(self, corpus_tk):\n",
    "        freq = {}\n",
    "        for i in range(len(corpus_tk)-1):\n",
    "            pair = (corpus_tk[i], corpus_tk[i+1])\n",
    "            if self._is_pair_viable(pair):\n",
    "                if pair in freq:\n",
    "                    freq[pair] += 1\n",
    "                else:\n",
    "                    freq[pair] = 1\n",
    "        if len(freq)>0:\n",
    "            return max(freq, key=freq.get)\n",
    "        else:\n",
    "            return {}\n",
    "        \n",
    "    def _is_pair_viable(self, pair):\n",
    "        merge = ''.join(pair)\n",
    "        return ' ' not in merge[1:-1]\n",
    "\n",
    "    def _reset(self):\n",
    "        self.merge_ops = []\n",
    "        self.vocab = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac13b5f-9e90-472b-bb00-7cdacf280845",
   "metadata": {},
   "source": [
    "## 3. Testing Tokenizer\n",
    "\n",
    "#### 3.1 Testing a tokenizer with 10 added subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f065e0-75fa-40fe-9481-e9d95c267847",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPETokenizer(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4307ea-6b66-403a-b521-20258ea3e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.fit(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461766a7-44ed-449a-a053-2a2a2d62353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat is sleeping. -> |th|e |c|a|t |i|s |s|l|e|e|p|in|g|.| \n",
      "the cat is sleeping. -> [86, 85, 80, 32, 88, 35, 89, 24, 9, 11, 11, 0, 91, 15, 75]\n",
      "# tokens: 15\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the cat is sleeping.'\n",
    "tokens = bpe.tokenize(sentence)\n",
    "token_idxs = bpe.tokenize(sentence, return_idx=True)\n",
    "\n",
    "print(f\"{sentence} -> |{'|'.join(tokens)}| \")\n",
    "print(f\"{sentence} -> {token_idxs}\")\n",
    "print(f\"# tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8b666-3ba9-4fe7-93b5-87c67b8039a7",
   "metadata": {},
   "source": [
    "#### 3.2 Testing a tokenizer with 100 added subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b037c6-eb0e-4f44-94a3-a2f9696279c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPETokenizer(k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ee64d9-7290-4baf-bfbd-b873cb6e11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.fit(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75a5475-d235-4ee6-bea0-4a40504848ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat is sleeping. -> |the |c|at |is |s|le|e|p|ing|.| \n",
      "the cat is sleeping. -> [121, 80, 111, 116, 24, 130, 11, 0, 105, 75]\n",
      "# tokens: 10\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the cat is sleeping.'\n",
    "tokens = bpe.tokenize(sentence)\n",
    "token_idxs = bpe.tokenize(sentence, return_idx=True)\n",
    "\n",
    "print(f\"{sentence} -> |{'|'.join(tokens)}| \")\n",
    "print(f\"{sentence} -> {token_idxs}\")\n",
    "print(f\"# tokens: {len(tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

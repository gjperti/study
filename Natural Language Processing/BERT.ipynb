{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1401d46a-5a13-42b4-9f78-385fdd75b961",
   "metadata": {},
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "This notebook explores the hugely influential [BERT sentence encoder](https://arxiv.org/pdf/1810.04805) proposed by Google's team in 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b051a8-4429-4111-82ab-31fe728f301e",
   "metadata": {},
   "source": [
    "## 1. What is BERT?\n",
    "\n",
    "BERT is an encoder-only transformer architecture that takes in a chunk of text, breaks it down into tokens and outputs one contextual embedding for each token. These embeddings will hopefully contain some information about the semantics and grammatical structure of human-written text, and can then be used with transfer learning to perform different kinds of tasks (e.g.sentiment analysis).\n",
    "\n",
    "The usefulness of BERT comes form the fact that this huge model has already been pre-trained on many different corpora, which is usually an expensive and time-consuming endeavor. So we can just use these pre-trained weights, make some downstream modifications, train it on a more specific dataset and repurpose them cheaply to another task. This process is called 'fine-tuning'.\n",
    "\n",
    "## 2. How is BERT trained?\n",
    "\n",
    "BERT is trained self-supervisedly to take two sentences as inputs ($A$, $B$) and perform two simultenous tasks:\n",
    "1. **Next Sentence Prediction (NSP)**: predicting whether $B$ follows $A$ (i.e. if the sentences are next to each other in the corpus)\n",
    "2. **Masked Language Model (MLM)**: masking a few of the tokens from these sentences and using the model to predict which tokens were masked (e.g. 'I [MASK] your father' $\\rightarrow$ model([MASK]) = 'am')\n",
    "\n",
    "The inputs take the following format:\n",
    "\n",
    "$$\n",
    "[\\text{CLS}] \\;\\; t_1^A \\;\\; t_2^A \\;\\; t_3^A \\;\\; ... \\;\\; t_{|A|}^A \n",
    "\\;\\; [\\text{SEP}] \\;\\; t_1^B \\;\\; t_2^B \\;\\; t_3^B \\;\\; ... \\;\\; t_{|B|}^B \n",
    "$$\n",
    "\n",
    "Here, $t_i^X$ represents the $i^{th}$ token of sentence X, [SEP] represents the special token used to separate the two sentences and [CLS] is another special token used in the beginning of the sequence, and whose goal is to encapsule some general encoding of the entire input (i.e. not only on a token level).\n",
    "\n",
    "<center><img src=\"data/BERT_architecture.png\" width=300 height=300></center>\n",
    "\n",
    "\n",
    "## 3. What data was BERT trained on?\n",
    "\n",
    "The original BERT model was trained only with English sentence-pairs taken from two different corpus: the BookCorpus (800M words) and English Wikipedia (2,500M words). There have been many updates done to BERT since its release, some focusing on language specific versions (e.g. CamemBERT for French) and multilingual versions (e.g. mBERT), but in this notebook, I'll use the original English-only model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e43ae-b190-4103-959f-53d079474fdd",
   "metadata": {},
   "source": [
    "## 4. Using BERT\n",
    "\n",
    "The most simple way to access BERT's architecture, tokenizer and pre-trained weights is using Hugging Face's library.\n",
    "\n",
    "### 4.1 - BERT Tokenization\n",
    "\n",
    "First, we load BERT's tokenizer, which will be used to convert a sentence into a sequence of token indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b18ff4-bd8c-4b76-9e2d-60641fb5ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886d599-7538-430d-a16c-2c5f3572bcae",
   "metadata": {},
   "source": [
    "We can now check which tokens some of the token indexes represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004311ab-d12b-459f-8a95-5354a6500dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:       0, token:  [PAD]\n",
      "idx:     101, token:  [CLS]\n",
      "idx:     102, token:  [SEP]\n",
      "idx:     103, token:  [MASK]\n",
      "idx:   10030, token:  vacant\n",
      "idx:    3029, token:  organization\n",
      "idx:    5142, token:  concern\n"
     ]
    }
   ],
   "source": [
    "for idx in [0, 101, 102, 103, 10030, 3029, 5142]:\n",
    "    token = tokenizer.convert_ids_to_tokens(idx)\n",
    "    print(f'idx: {idx:7}, token: ', token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a8438-2cdd-4832-ac90-579394cfb0ca",
   "metadata": {},
   "source": [
    "When applying the tokenizer to a sentence, we get a dictionary with:\n",
    "- **input_ids**: the token indexes (including [CLS] and [SEP])\n",
    "- **token_type_ids**: the sentence indexes (0 for first, 1 for second)\n",
    "- **attention_mask**: 1 if the token should be used in the attention calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110d085a-bf1d-4ff1-b9e3-b95d0b3c04e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "tensor([[  101,  1045,  2001, 24907,   999,   102,  1045, 10749,  2300,  1024,\n",
      "          1040,   102]])\n",
      "\n",
      "token_type_ids: \n",
      "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "attention_mask: \n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"I was thirsty!\", \"I drank water :D\", return_tensors=\"pt\")\n",
    "for key, value in inputs.items():\n",
    "    print(f'{key}: \\n{value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff659529-44da-404a-b97b-6b1cebd7fe18",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "We can now see what tokens are being fed to BERT. Note that, since we are using the 'uncased' version of the model, the sentences are all converted to lower case before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5911981a-508a-4844-bc8f-54baaaf089b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: [CLS] i was thirsty ! [SEP] i drank water : d [SEP]\n"
     ]
    }
   ],
   "source": [
    " tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    " print('tokens:', ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674e798-da1b-4758-addd-2f5ec337609b",
   "metadata": {},
   "source": [
    "### 4.2 - BERT Embeddings\n",
    "\n",
    "First, we can import BERT from the Hugging Face repository. We will work with the lighted version (base) that disregards lower/upper case (uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0dad34-0184-4162-a4ce-6dd715f51cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193123b0-e17d-4d1e-a3f2-7e8fcf94cbd8",
   "metadata": {},
   "source": [
    "BERT now uses this _inputs_ dictionary to build three separate embeddings:\n",
    "- **position embeddings**: takes token position in the sequence into account (0,1,2,3,...)\n",
    "- **token embeddings**: one different embedding for each token\n",
    "- **sentence embeddings**: one different embedding for each sentence (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4052c8df-c9b9-4aa3-8659-2a16fdea7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim. of position embeddings: [512, 768]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
       "          6.8312e-04,  1.5441e-02],\n",
       "        [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
       "          2.9753e-02, -5.3247e-03],\n",
       "        [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
       "          1.8741e-02, -7.3140e-03],\n",
       "        ...,\n",
       "        [ 1.7418e-02,  3.4903e-03, -9.5621e-03,  ...,  2.9599e-03,\n",
       "          4.3435e-04, -2.6949e-02],\n",
       "        [ 2.1687e-02, -6.0216e-03,  1.4736e-02,  ..., -5.6118e-03,\n",
       "         -1.2590e-02, -2.8085e-02],\n",
       "        [ 2.6413e-03, -2.3298e-02,  5.4922e-03,  ...,  1.7537e-02,\n",
       "          2.7550e-02, -7.7656e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Position embeddings\n",
    "pos_embed = model.embeddings.position_embeddings\n",
    "\n",
    "# Embedding dimension = max sequence length x dimension\n",
    "print(f'dim. of position embeddings: {list(pos_embed.weight.shape)}')\n",
    "\n",
    "# Embeddings print\n",
    "pos_embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4d04d7-60b5-44e7-b031-852ea66b6fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim. of token embeddings: [30522, 768]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
       "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
       "        ...,\n",
       "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
       "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
       "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token embeddings\n",
    "token_embed = model.embeddings.word_embeddings\n",
    "\n",
    "# Embedding dimension = size of vocabulary x dimension\n",
    "print(f'dim. of token embeddings: {list(token_embed.weight.shape)}')\n",
    "\n",
    "# Embeddings print\n",
    "token_embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a0d9fa-1c88-4273-aca5-fa3e1f31dfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim. of sentence embeddings: [2, 768]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0004,  0.0110,  0.0037,  ..., -0.0066, -0.0034, -0.0086],\n",
       "        [ 0.0011, -0.0030, -0.0032,  ...,  0.0047, -0.0052, -0.0112]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence embeddings\n",
    "sent_embed = model.embeddings.token_type_embeddings\n",
    "\n",
    "# Embedding dimension = 2 x dimension\n",
    "print(f'dim. of sentence embeddings: {list(sent_embed.weight.shape)}')\n",
    "\n",
    "# Embeddings print\n",
    "sent_embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e793c0-9c97-448c-bfaa-c789f7727658",
   "metadata": {},
   "source": [
    "The input to the BERT network is a sum of each of these embeddings for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbe508-bfb7-45ee-9e00-4605a0c8fe09",
   "metadata": {},
   "source": [
    "### 4.3 - BERT Outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7e868-2abf-4442-910f-661b8e4dcc24",
   "metadata": {},
   "source": [
    "Let's first take a look at BERT's architecture.\n",
    "It consists of:\n",
    "1. **Embedding**: the three embeddings discussed in 4.2\n",
    "2. **BERT Layers**: 12 (base version) BERT Layers in series, and each of these layers contain a multiheaded attention layer, skip connections, layer normalization and a feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6363646e-ed5e-43ef-803e-207c0d157305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31537b6a-43b0-473f-a1d9-9f6172e37191",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "In this implementation, the BERT model receives the _inputs_ dictionary and outputs two sets of vectors:\n",
    "- **last_hidden_state**: the the final contextual embeddings for each input token (including [CLS] and [SEP])\n",
    "- **pooler_output**: a layer normalization of the contextual embedding for [CLS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b9bc5b-745a-4c08-a737-59cf8163efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87c110-b1cb-4df5-907c-0ccb7dda0e62",
   "metadata": {},
   "source": [
    "Let's now print the first 10 dimensions of the output embedding of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38f1a4de-0a82-40de-b9d9-8dcb1c31ac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]      -> [-0.23, 0.37, -0.34, -0.35, -0.91, 0.01, 0.67, 0.64, 0.1, -0.32]\n",
      "\n",
      "i          -> [0.11, 0.19, 0.05, 0.25, -0.13, 0.56, -0.07, 1.19, -0.03, -0.78]\n",
      "\n",
      "was        -> [0.45, 0.47, 0.29, 0.36, 0.07, 0.35, 0.37, 0.82, -0.54, -0.47]\n",
      "\n",
      "thirsty    -> [0.88, 0.08, -0.0, -0.03, 1.2, 0.08, 0.37, 0.98, -0.37, 0.08]\n",
      "\n",
      "!          -> [0.22, 0.1, -0.11, -0.29, 0.19, 0.44, 0.61, 0.16, -0.67, -0.38]\n",
      "\n",
      "[SEP]      -> [0.6, 0.19, -0.24, 0.49, -0.5, -0.79, 0.73, -0.06, 0.46, 0.15]\n",
      "\n",
      "i          -> [0.05, 0.29, 0.28, 0.03, -0.71, 0.5, 0.35, 1.22, 0.34, -0.32]\n",
      "\n",
      "drank      -> [0.16, 0.34, 0.4, -0.17, -0.33, 0.24, 0.6, 0.89, 0.01, -0.3]\n",
      "\n",
      "water      -> [0.22, 0.05, 0.1, -0.23, 0.09, -0.27, 0.31, 0.82, -0.35, 0.01]\n",
      "\n",
      ":          -> [0.11, 0.39, -0.11, -0.63, -0.53, 0.44, 0.71, 0.21, 0.08, 0.23]\n",
      "\n",
      "d          -> [0.15, 0.93, 0.78, -0.26, -0.7, 0.4, 1.27, -0.72, 0.36, -0.42]\n",
      "\n",
      "[SEP]      -> [0.75, 0.15, -0.57, 0.65, -0.7, -0.76, 0.75, -0.26, 0.7, 0.01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(inputs['input_ids'][0]):\n",
    "    context_embed = outputs.last_hidden_state[0,i,0:10].tolist()\n",
    "    context_embed = [round(emb,2) for emb in context_embed]\n",
    "    print(f'{tokenizer.convert_ids_to_tokens([idx])[0]:10} -> {context_embed}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31091b1f-4bab-4f45-af3b-8b184eb9294e",
   "metadata": {},
   "source": [
    "Let's check the pooler_output now. It is simply a layer normalization from [CLS]'s contextual embedding from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ca860b-f0ae-400b-ac9f-58b541a97228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9803, -0.7672, -0.9958,  0.9636,  0.9004, -0.5488,  0.9910,  0.6867,\n",
       "        -0.9878, -1.0000], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3c020-9cdf-4a71-bb2c-74e43dd0f045",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "There are 4 kinds of downstream tasks that we can use with BERT:\n",
    "1. **1 sentence, a label for each token**: e.g. named entity recognition\n",
    "    - use last_hidden_states\n",
    "3. **1 sentence, a label for the entire sentence**: e.g. sentiment analysis\n",
    "    - use pooler_output\n",
    "5. **2 sentences, a label for each token**: e.g. information retrieval\n",
    "    - use last_hidden_states\n",
    "6. **2 sentences, a label for the pair**: e.g. find duplicate questions\n",
    "    - use pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711dcaa-1508-4440-884e-a5073670a6b3",
   "metadata": {},
   "source": [
    "## 5 - Fine-Tuning BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896bdf5-c8a6-4f15-a3c8-07f9da02f840",
   "metadata": {},
   "source": [
    "We will now fine-tune BERT on sentiment analysis data, i.e. data that consists of sentences and a label of 0 (negative), 1 (neutral) or 2(positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3153cfd7-43b6-47b3-9a6a-bfe5dc39043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fee27-1962-4b42-be0a-1cad80a31434",
   "metadata": {},
   "source": [
    "We will use MTEB's twitter sentiment analysis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53991b48-6fae-4762-8457-9629c8daa959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 2000\n"
     ]
    }
   ],
   "source": [
    "label_dic = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}\n",
    "\n",
    "train = load_dataset(\"mteb/tweet_sentiment_extraction\", split=\"train[:2000]\")\n",
    "print(f'train size: {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37c022-3143-4e55-a6c2-e3a0bf5c1f71",
   "metadata": {},
   "source": [
    "___\n",
    "The classifier will be simply a logistic regression that will be fitted on top of the **pooler_output** vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdbb752a-20c0-483c-99ae-17fab621b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Linear(768,3)\n",
    "\n",
    "optim = torch.optim.Adam(classifier.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364290c-fffd-43ef-8403-7fdc0aec77cc",
   "metadata": {},
   "source": [
    "To make training faster, we will freeze the parameters from BERT itself. So, it will act as a static feature extractor, and only the logistic regression will be fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bfa5f9d-d40d-4452-bb6f-360227e1ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389983b-a6a3-44a7-a605-82345e264b94",
   "metadata": {},
   "source": [
    "Training the classifier by first passing the sentences through BERT, taking the pooler_output vector and then applying the classifier to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40f623ca-b9f4-492b-9ed5-c881062e7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:26<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [1/10] | last batch training loss: 1.0779420137405396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:25<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [2/10] | last batch training loss: 0.9721745848655701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:25<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [3/10] | last batch training loss: 0.9489535093307495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:24<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [4/10] | last batch training loss: 0.9208956360816956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:24<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [5/10] | last batch training loss: 0.8892520666122437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [6/10] | last batch training loss: 0.8570788502693176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:24<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [7/10] | last batch training loss: 0.8284878134727478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:24<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [8/10] | last batch training loss: 0.8041089177131653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:24<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [9/10] | last batch training loss: 0.7828696966171265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:24<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [10/10] | last batch training loss: 0.7639827132225037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "n_epochs = 10\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    # For each batch in the training set\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        # Get text and label\n",
    "        texts = batch['text']\n",
    "        y = batch['label']\n",
    "\n",
    "        # Applying BERT to texts and getting pooler_output\n",
    "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        cls_token = outputs.pooler_output\n",
    "\n",
    "        # Applying the classifier to pooler_output\n",
    "        y_pred = classifier(cls_token)\n",
    "\n",
    "        # Finding loss\n",
    "        loss = loss_fn(y_pred,y)\n",
    "\n",
    "        # Updating weights through backprop\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    print(f'epoch: [{i+1}/{n_epochs}] | last batch training loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ca8f0-86cb-4691-bff8-156fcbda7714",
   "metadata": {},
   "source": [
    "Now, let's test the trained model on some made-up sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d640461-20fc-463d-a1c2-801d49af9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'im very sad at the whole situation',\n",
    "    'i woke up feeling amazing',\n",
    "    'theres going to be a full eclipse 2day',\n",
    "    'i wish i could have a different life sometimes. it gets hard',\n",
    "    'this is great news!',\n",
    "    'I just got a raise :o'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d8e46e8-fd08-4593-8e27-784b20c793f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im very sad at the whole situation -> negative\n",
      "\n",
      "i woke up feeling amazing -> positive\n",
      "\n",
      "theres going to be a full eclipse 2day -> neutral\n",
      "\n",
      "i wish i could have a different life sometimes. it gets hard -> negative\n",
      "\n",
      "this is great news!  -> positive\n",
      "\n",
      "I just got a raise :o -> positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    \n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    cls_token = outputs.pooler_output\n",
    "    logprobs = classifier(cls_token)\n",
    "    predicted_label = F.softmax(logprobs, dim=-1).argmax()\n",
    "    print(f'{sentence:20} -> {label_dic[predicted_label.item()]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2a857-ebac-4dd0-a80b-b8faf088e2ca",
   "metadata": {},
   "source": [
    "From the examples above, it is clear that BERT has at the very least provided a good 'automatic feature engineering procedure' for the classsification head."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

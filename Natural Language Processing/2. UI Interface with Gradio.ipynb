{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "530b4a95-aa45-4e56-abce-981ae584d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import io\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import json\n",
    "\n",
    "load_dotenv('../.env', override=True)\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ab55f-0741-4c07-a09f-f208699d86a8",
   "metadata": {},
   "source": [
    "# 1. Paper Summarizer in Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d7b32ec3-acb8-4068-81ed-76a065515e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(url):\n",
    "    content = get_pdf_content(url)\n",
    "    messages = [\n",
    "        {'role': 'system',\n",
    "         'content': \"You will summarize the user's paper in Brazilian Portuguese. The language should be easy and informal.\"},\n",
    "        \n",
    "        {'role': 'user',\n",
    "         'content': f'Paper content:\\n {content} \\n\\n Summarization:'}\n",
    "    ]\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        messages = messages,\n",
    "        model='gpt-4o-mini',\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "957e1805-f9a7-4403-9f80-d7a5c3ea8691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7882\n",
      "* Running on public URL: https://bd65fd69d075b25519.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bd65fd69d075b25519.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(fn=message_gpt, \n",
    "                    inputs=[gr.Textbox(label=\"Paper URL\")],\n",
    "                    outputs=gr.Textbox(label=\"Br Translation\"),\n",
    "                    flagging_mode=\"never\")\n",
    "\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb91b9-1fa9-4643-a6b8-8ccba198d483",
   "metadata": {},
   "source": [
    "# 2. Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f8270429-c14d-4043-aa60-92c8fc3eec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \n",
    "    system_message = \"You will always try to contradict the user.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model='gpt-4o-mini', messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb7543bc-db67-4414-8b0b-a653cc4369bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7895\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7895/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type='messages').launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fc8b0-f9e7-4a7c-b20d-595fd0fd0202",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60619ad0-6575-417e-a6eb-37ee8fdd9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'shoe': '$100', \n",
    "        'bag': '$32',\n",
    "        'shirt': '$50'}\n",
    "\n",
    "def get_price(item):\n",
    "    item = item.lower()\n",
    "    if item in data:\n",
    "        return data[item]\n",
    "    return \"I don't know\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a50cf03d-8ad1-4bf7-9900-cc44b541383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_function = {\n",
    "    \"name\": \"get_price\",\n",
    "    \"description\": \"Gets item's price.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"item\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"an object for sale in a clothing store, such as a hat or a shoe. it should be a single word.\"\n",
    "            }\n",
    "        },\n",
    "    \"required\": [\"item\"],\n",
    "    \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [{'type': 'function', 'function': price_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aaf50a83-d315-4abe-9297-a13eec68d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \n",
    "    system_message = \"You are a useful assistant. Reply briefly. If you don't know something, tell the user you don't know it.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason == 'tool_calls':\n",
    "        message = response.choices[0].message\n",
    "        tool_call = message.tool_calls[0]\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        item = arguments['item']\n",
    "        response = {\n",
    "            'role': 'tool',\n",
    "            'content': json.dumps({'item': item, 'price': get_price(item)}),\n",
    "            'tool_call_id': message.tool_calls[0].id\n",
    "        }\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        print(response)\n",
    "        response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages)\n",
    "        \n",
    "    return response.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "32953bc9-16fe-4cdc-9d32-718ae119f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7894\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7894/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'tool', 'content': '{\"item\": \"shoe\", \"price\": \"$100\"}', 'tool_call_id': 'call_2oZWaaTPHwLOu0EQYelZU47V'}\n",
      "{'role': 'tool', 'content': '{\"item\": \"handbag\", \"price\": \"I don\\'t know\"}', 'tool_call_id': 'call_o1OurG2m7IUqvHBnIG4xFCdr'}\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type='messages').launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e63ee-65b0-41ba-99c4-36a20df8e63f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed482ef-ebf6-4f37-99ff-89cf189b8b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9f02fdf-3f54-4310-b608-4833784eea58",
   "metadata": {},
   "source": [
    "# Multimodal Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dba8dca3-4e27-496b-88aa-c4f8d7caef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_image(food):\n",
    "\n",
    "    # Request the image as base64\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=food,\n",
    "        size=\"1024x1024\",\n",
    "        response_format=\"b64_json\"\n",
    "    )\n",
    "\n",
    "    image_base64 = response.data[0].b64_json\n",
    "    image_bytes = base64.b64decode(image_base64)\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d4c2673b-d6d5-4041-a663-aeb9f5a4dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(food):\n",
    "    return f'<{food}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36e143ab-d6a3-4085-ba64-91c9d8fe33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chef = {\n",
    "    \"name\": \"chef\",\n",
    "    \"description\": \"Gets a food description\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"food\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"the description of a food or beverage.\"\n",
    "            }\n",
    "        },\n",
    "    \"required\": [\"item\"],\n",
    "    \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [{'type': 'function', 'function': chef}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fb5a2a76-d3d5-468b-815d-4fd81fdf11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "        You are the assistant to a virtual restaurant. \n",
    "        When the user asks for a food or beverage, no matter how crazy, you will provide him.\n",
    "        \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason == 'tool_calls':\n",
    "        message = response.choices[0].message\n",
    "        tool_call = message.tool_calls[0]\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        food = arguments['food']\n",
    "        response = {\n",
    "            'role': 'assistant',\n",
    "            'content': get_image(food),\n",
    "            'tool_call_id': message.tool_calls[0].id\n",
    "        }\n",
    "        messages.append(response)\n",
    "        response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages)\n",
    "        \n",
    "    return response.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3affdd80-4c3c-4553-9fa2-2aafe98cd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7901\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7901/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1633, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py\", line 861, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/45/871v_dk90997hj5g88_1z7tr0000gn/T/ipykernel_16108/4141311432.py\", line 23, in chat\n",
      "    response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 968, in request\n",
      "    request = self._build_request(options, retries_taken=retries_taken)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 547, in _build_request\n",
      "    return self._client.build_request(  # pyright: ignore[reportUnknownMemberType]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/httpx/_client.py\", line 378, in build_request\n",
      "    return Request(\n",
      "           ^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/httpx/_models.py\", line 408, in __init__\n",
      "    headers, stream = encode_request(\n",
      "                      ^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/httpx/_content.py\", line 216, in encode_request\n",
      "    return encode_json(json)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/httpx/_content.py\", line 177, in encode_json\n",
      "    body = json_dumps(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/json/__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/json/encoder.py\", line 200, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/json/encoder.py\", line 258, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/json/encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type PngImageFile is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type='messages').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "987b6981-bf60-434d-9ac3-8b6be02a24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/components/chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7902\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7902/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1633, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py\", line 869, in _submit_fn\n",
      "    history = self._append_message_to_history(response, history, \"assistant\")\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py\", line 800, in _append_message_to_history\n",
      "    message_dicts = self._message_as_message_dict(message, role)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py\", line 838, in _message_as_message_dict\n",
      "    for x in msg.get(\"files\", []):\n",
      "             ^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define the chatbot function\n",
    "def respond(message, history):\n",
    "    if \"cat\" in message.lower():\n",
    "        # Reply with text + image in the same message\n",
    "        return [(\"Here’s a cat I found!\", \"cat.png\")]\n",
    "    elif \"dog\" in message.lower():\n",
    "        # Reply with only an image\n",
    "        return [(None, \"dog.png\")]\n",
    "    else:\n",
    "        # Normal text-only reply\n",
    "        return [(\"You said: \" + message, None)]\n",
    "\n",
    "# Create the ChatInterface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me something...\"),\n",
    "    title=\"Chatbot with Images\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126dec3-86d4-4e8a-b778-cc6f12193493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

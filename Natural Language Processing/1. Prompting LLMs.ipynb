{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91eed376-127b-4aa8-a946-6f692095f069",
   "metadata": {},
   "source": [
    "# Prompting LLMs\n",
    "\n",
    "There are three ways to prompt LLMs and get a response:\n",
    "\n",
    "1. **User Interfaces (UI)**: exchanging essages with a chatbot through a dedicated website. Either free or with a monthly subscription.\n",
    "2. **APIs**: sending messages with a bit more control via APIs and getting a reply. Charged per token.\n",
    "3. **Running Locally**: downloading open-source models and running them with full control on a machine. Free.\n",
    "\n",
    "Below is a summary for some of the commonly used models:\n",
    "\n",
    "|Model Family|Developed by|User Interface|Open Source?|\n",
    "|--|--|--|--|\n",
    "|GPT|Open AI|[chatgpt.com](chatgpt.com)| Only GPT-OSS|\n",
    "|Gemini|Google|[gemini.google.com](https://gemini.google.com/app)| No |\n",
    "|Llama|Meta|[meta.ai](https://www.meta.ai)| Yes |\n",
    "|Qwen|Alibaba|[chat.qwen.ai](https://chat.qwen.ai)| Yes |\n",
    "|Mixtral|Mistral|[chat.mistral.ai](https://chat.mistral.ai)| Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede2815-cabd-4893-ac0e-ca242090fa3b",
   "metadata": {},
   "source": [
    "# 1 - Interacting with GPT via API\n",
    "\n",
    "Calling OpenAI's API is really simple: after adding $5 to the account and generating a token, the token needs to be added to a .env file (for security reasons). Then, the token is imported via load_dotenv() and the system and user prompts are crafted and sent to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "34519056-c352-4dad-a445-55bddaf1c465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Imagine a big library filled with lots of books about all kinds of animals. Now, pretend there's a friendly robot parrot who really loves to listen to the stories from those books. This robot learns tons of words and stories until it gets really good at chatting! \n",
      "\n",
      "A large language model is like that parrot. It listens to lots and lots of writing (like reading all those books) so it can put together sentences and talk to you about anything – like asking about your favorite animals or telling you a fun story!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Load OpenAI API keys from .env\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()\n",
    "\n",
    "# Craft the message that will be sent to the API\n",
    "messages = [\n",
    "\n",
    "    # System prompt\n",
    "    {'role': 'system', \n",
    "     'content': \"\"\"\n",
    "         You are a science expert that can simply concepts really well. \n",
    "         Address the user's questions and explain like he is 5 years old.\n",
    "         Make the explanations short and intuitive, preferably using animals as analogies.\n",
    "         \"\"\"},\n",
    "    \n",
    "    # User prompt\n",
    "    {'role': 'user', \n",
    "     'content': 'What is a large language model?'}\n",
    "    \n",
    "]\n",
    "\n",
    "# Pick arguments (temperature, model etc) and pass the message to get a response\n",
    "response = openai.chat.completions.create(\n",
    "\n",
    "    # choose model\n",
    "    model=\"gpt-4o-mini\",\n",
    "\n",
    "    # temperature (must be <= 2)\n",
    "    temperature=1.5,\n",
    "\n",
    "    # pass messages (user + system prompt)\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579a779-c34a-4fd9-a691-1ae39f9c1fe1",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "It's also possible to get the probabilities of the top-k candidates for the next token using the argument _logprobs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a8ad0d37-d25f-461b-a0dd-35116d8069ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Albert Einstein.\n"
     ]
    }
   ],
   "source": [
    "# Pick arguments (temperature, model etc) and pass the message to get a response\n",
    "response = openai.chat.completions.create(\n",
    "\n",
    "    # choose model\n",
    "    model=\"gpt-4o-mini\",\n",
    "\n",
    "    # temperature (must be <= 2)\n",
    "    temperature=0.01,\n",
    "\n",
    "    # pass messages (user + system prompt)\n",
    "    messages= [{'role': 'user', \n",
    "               'content': 'Who came up with the theory of General Relativity? Give me just the name.'}],\n",
    "\n",
    "    # will return log-probabilities\n",
    "    logprobs=True,\n",
    "\n",
    "    # will return the probs for the 5 most likely alternatives for each token generated\n",
    "    top_logprobs=5,\n",
    "\n",
    "    # max tokens in the repsonse\n",
    "    max_tokens=5\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "94033d6c-e663-4e53-983f-17bfa587cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:    'Albert'        (log prob. = -0.0)\n",
      "2:    'Ein'           (log prob. = -11.3)\n",
      "3:    'Al'            (log prob. = -16.0)\n",
      "4:    ' Albert'       (log prob. = -17.8)\n",
      "5:    'Isa'           (log prob. = -19.0)\n",
      "Chosen token: 'Albert'\n",
      " \n",
      "\n",
      "1:    ' Einstein'     (log prob. = 0.0)\n",
      "2:    'Ein'           (log prob. = -18.6)\n",
      "3:    ' Ein'          (log prob. = -20.5)\n",
      "4:    ' ein'          (log prob. = -21.6)\n",
      "5:    ' EIN'          (log prob. = -21.9)\n",
      "Chosen token: ' Einstein'\n",
      " \n",
      "\n",
      "1:    '.'             (log prob. = -0.0)\n",
      "2:    '<|end|>'       (log prob. = -4.4)\n",
      "3:    '。'             (log prob. = -14.5)\n",
      "4:    '<|end|>'       (log prob. = -14.8)\n",
      "5:    '.\\n'           (log prob. = -14.9)\n",
      "Chosen token: '.'\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token_info in response.choices[0].logprobs.content:\n",
    "    chosen_token = token_info.token\n",
    "    \n",
    "    # show top-k alternatives\n",
    "    for n, alt in enumerate(token_info.top_logprobs):\n",
    "        token = alt.token\n",
    "        logprob = alt.logprob\n",
    "        \n",
    "        print(f\"{n+1}:    {token!r:15} (log prob. = {logprob:0.1f})\")\n",
    "    print(f\"Chosen token: {chosen_token!r}\")\n",
    "    print(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64af41-93d4-430f-810b-7b6ab38bf463",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "If the temperature is high, the likelihood of the LLM picking less likely tokens increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c26b974-8362-4c28-bf5d-3d772c2326f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Experiencing love, connection, growth, purpose\n",
      "___\n",
      "\n",
      "1:    'Seek'          (log prob. = -1.1)\n",
      "2:    'P'             (log prob. = -1.4)\n",
      "3:    'To'            (log prob. = -2.5)\n",
      "4:    'Experience'    (log prob. = -2.6)\n",
      "5:    'Find'          (log prob. = -3.0)\n",
      "Chosen token: 'Exper'\n",
      " \n",
      "\n",
      "1:    'ien'           (log prob. = -0.1)\n",
      "2:    'iences'        (log prob. = -2.6)\n",
      "3:    'iential'       (log prob. = -9.0)\n",
      "4:    'ience'         (log prob. = -11.0)\n",
      "5:    'iene'          (log prob. = -15.5)\n",
      "Chosen token: 'ien'\n",
      " \n",
      "\n",
      "1:    'cing'          (log prob. = 0.0)\n",
      "2:    'cer'           (log prob. = -18.5)\n",
      "3:    'c'             (log prob. = -18.8)\n",
      "4:    'cin'           (log prob. = -20.0)\n",
      "5:    'cers'          (log prob. = -20.5)\n",
      "Chosen token: 'cing'\n",
      " \n",
      "\n",
      "1:    ' love'         (log prob. = -0.2)\n",
      "2:    ' joy'          (log prob. = -2.2)\n",
      "3:    ','             (log prob. = -3.1)\n",
      "4:    ' connection'   (log prob. = -4.0)\n",
      "5:    ' connections'  (log prob. = -5.5)\n",
      "Chosen token: ' love'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -6.5)\n",
      "3:    ';'             (log prob. = -16.5)\n",
      "4:    ' through'      (log prob. = -17.4)\n",
      "5:    ' while'        (log prob. = -18.3)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' growth'       (log prob. = -0.3)\n",
      "2:    ' connection'   (log prob. = -2.0)\n",
      "3:    ' joy'          (log prob. = -2.8)\n",
      "4:    ' purpose'      (log prob. = -3.5)\n",
      "5:    ' learning'     (log prob. = -5.6)\n",
      "Chosen token: ' connection'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -11.5)\n",
      "3:    ';'             (log prob. = -17.4)\n",
      "4:    ',and'          (log prob. = -21.8)\n",
      "5:    ' with'         (log prob. = -22.1)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' and'          (log prob. = -0.0)\n",
      "2:    ' growth'       (log prob. = -5.8)\n",
      "3:    ' personal'     (log prob. = -6.1)\n",
      "4:    ' purpose'      (log prob. = -8.6)\n",
      "5:    ' seeking'      (log prob. = -9.6)\n",
      "Chosen token: ' growth'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' together'     (log prob. = -5.4)\n",
      "3:    '.'             (log prob. = -8.8)\n",
      "4:    ' and'          (log prob. = -10.5)\n",
      "5:    ';'             (log prob. = -12.3)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' purpose'      (log prob. = -0.3)\n",
      "2:    ' and'          (log prob. = -2.4)\n",
      "3:    ' joy'          (log prob. = -3.1)\n",
      "4:    ' understanding' (log prob. = -3.5)\n",
      "5:    ' fulfillment'  (log prob. = -3.5)\n",
      "Chosen token: ' purpose'\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature = 2\n",
    "prompt = 'What is the meaning of life? Reply in 5 words.'\n",
    "\n",
    "###################\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=temperature,\n",
    "    messages= [{'role': 'user', \n",
    "               'content': prompt}],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)\n",
    "print('___\\n')\n",
    "\n",
    "for token_info in response.choices[0].logprobs.content:\n",
    "    chosen_token = token_info.token\n",
    "    for n, alt in enumerate(token_info.top_logprobs):\n",
    "        token = alt.token\n",
    "        logprob = alt.logprob\n",
    "        \n",
    "        print(f\"{n+1}:    {token!r:15} (log prob. = {logprob:0.1f})\")\n",
    "    print(f\"Chosen token: {chosen_token!r}\")\n",
    "    print(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb388c3-035b-4ed8-befb-2b770783efd7",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "If the temperature is low, the model will gravitate towards the most likely tokens every time, making the answer constant no matter how any times it is asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a7036a84-c4d7-4947-80b2-c677b55274e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Seek purpose, connection, and happiness.\n",
      "___\n",
      "\n",
      "1:    'Seek'          (log prob. = -1.2)\n",
      "2:    'P'             (log prob. = -1.6)\n",
      "3:    'Experience'    (log prob. = -2.4)\n",
      "4:    'To'            (log prob. = -2.7)\n",
      "5:    'Find'          (log prob. = -3.1)\n",
      "Chosen token: 'Seek'\n",
      " \n",
      "\n",
      "1:    ' purpose'      (log prob. = -1.2)\n",
      "2:    ' happiness'    (log prob. = -1.2)\n",
      "3:    ' joy'          (log prob. = -1.4)\n",
      "4:    ' connection'   (log prob. = -2.7)\n",
      "5:    ' love'         (log prob. = -3.2)\n",
      "Chosen token: ' purpose'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -7.5)\n",
      "3:    ';'             (log prob. = -9.8)\n",
      "4:    ' through'      (log prob. = -11.0)\n",
      "5:    ' in'           (log prob. = -13.9)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' connection'   (log prob. = -1.1)\n",
      "2:    ' create'       (log prob. = -1.2)\n",
      "3:    ' love'         (log prob. = -1.5)\n",
      "4:    ' connect'      (log prob. = -3.0)\n",
      "5:    ' find'         (log prob. = -3.5)\n",
      "Chosen token: ' connection'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -14.6)\n",
      "3:    ';'             (log prob. = -16.9)\n",
      "4:    ':'             (log prob. = -21.4)\n",
      "5:    ' with'         (log prob. = -21.8)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' and'          (log prob. = -0.0)\n",
      "2:    ' love'         (log prob. = -3.7)\n",
      "3:    ' growth'       (log prob. = -4.9)\n",
      "4:    ' joy'          (log prob. = -5.3)\n",
      "5:    ' happiness'    (log prob. = -6.8)\n",
      "Chosen token: ' and'\n",
      " \n",
      "\n",
      "1:    ' happiness'    (log prob. = -1.2)\n",
      "2:    ' fulfillment'  (log prob. = -1.2)\n",
      "3:    ' growth'       (log prob. = -1.8)\n",
      "4:    ' joy'          (log prob. = -1.9)\n",
      "5:    ' understanding' (log prob. = -3.2)\n",
      "Chosen token: ' happiness'\n",
      " \n",
      "\n",
      "1:    '.'             (log prob. = -0.0)\n",
      "2:    '.\\n'           (log prob. = -14.3)\n",
      "3:    '.\\n\\n'         (log prob. = -17.5)\n",
      "4:    '!'             (log prob. = -18.1)\n",
      "5:    '.\"'            (log prob. = -18.1)\n",
      "Chosen token: '.'\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.01\n",
    "prompt = 'What is the meaning of life? Reply in 5 words.'\n",
    "\n",
    "###################\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=temperature,\n",
    "    messages= [{'role': 'user', \n",
    "               'content': prompt}],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)\n",
    "print('___\\n')\n",
    "\n",
    "for token_info in response.choices[0].logprobs.content:\n",
    "    chosen_token = token_info.token\n",
    "    for n, alt in enumerate(token_info.top_logprobs):\n",
    "        token = alt.token\n",
    "        logprob = alt.logprob\n",
    "        \n",
    "        print(f\"{n+1}:    {token!r:15} (log prob. = {logprob:0.1f})\")\n",
    "    print(f\"Chosen token: {chosen_token!r}\")\n",
    "    print(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba0c32-a23e-4f36-b1e1-dc8b09916217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

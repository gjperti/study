{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91eed376-127b-4aa8-a946-6f692095f069",
   "metadata": {},
   "source": [
    "# Prompting LLMs\n",
    "\n",
    "There are three ways to prompt LLMs and get a response:\n",
    "\n",
    "1. **User Interfaces (UI)**: exchanging essages with a chatbot through a dedicated website. Either free or with a monthly subscription.\n",
    "2. **APIs**: sending messages with a bit more control via APIs and getting a reply. Charged per token.\n",
    "3. **Running Locally**: downloading open-source models and running them with full control on a machine. Free.\n",
    "\n",
    "Below is a summary for some of the commonly used models:\n",
    "\n",
    "|Model Family|Developed by|User Interface|Open Source?|\n",
    "|--|--|--|--|\n",
    "|GPT|Open AI|[chatgpt.com](chatgpt.com)| Only GPT-OSS|\n",
    "|Gemini|Google|[gemini.google.com](https://gemini.google.com/app)| No |\n",
    "|Llama|Meta|[meta.ai](https://www.meta.ai)| Yes |\n",
    "|Qwen|Alibaba|[chat.qwen.ai](https://chat.qwen.ai)| Yes |\n",
    "|Mixtral|Mistral|[chat.mistral.ai](https://chat.mistral.ai)| Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede2815-cabd-4893-ac0e-ca242090fa3b",
   "metadata": {},
   "source": [
    "# 1 - Interacting with GPT via API\n",
    "\n",
    "Calling OpenAI's API is really simple: after adding $5 to the account and generating a token, the token needs to be added to a .env file (for security reasons). Then, the token is imported via load_dotenv() and the system and user prompts are crafted and sent to the API.\n",
    "\n",
    "The API call needs two arguments:\n",
    "- model: specifying which specific model will be prompted\n",
    "- messages: a list of dictionaries of the type {'role': XXX, 'content': XXX}, where:\n",
    "- - 'role' = 'system' means the content is the system prompt\n",
    "- - 'role' = 'user' means the content is the user's prompt\n",
    "- - 'role' = 'assistant' means the content is the model's past replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34519056-c352-4dad-a445-55bddaf1c465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Okay! Imagine a big, friendly giraffe who has read a whole jungle full of books. This giraffe remembers everything it read and can talk to you about it. \n",
      "\n",
      "A large language model is like that giraffe! It has learned from lots and lots of words and sentences, so when you ask it something, it uses all that knowledge to give you smart answers. Just like the giraffe can help you learn about animals, the language model helps you learn about many things using words!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Load OpenAI API keys from .env\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()\n",
    "\n",
    "# Craft the message that will be sent to the API\n",
    "messages = [\n",
    "\n",
    "    # System prompt\n",
    "    {'role': 'system', \n",
    "     'content': \"\"\"\n",
    "         You are a science expert that can simply concepts really well. \n",
    "         Address the user's questions and explain like he is 5 years old.\n",
    "         Make the explanations short and intuitive, preferably using animals as analogies.\n",
    "         \"\"\"},\n",
    "    \n",
    "    # User prompt\n",
    "    {'role': 'user', \n",
    "     'content': 'What is a large language model?'}\n",
    "    \n",
    "]\n",
    "\n",
    "# Pick arguments (temperature, model etc) and pass the message to get a response\n",
    "response = openai.chat.completions.create(\n",
    "\n",
    "    # choose model\n",
    "    model=\"gpt-4o-mini\",\n",
    "\n",
    "    # pass messages (user + system prompt)\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579a779-c34a-4fd9-a691-1ae39f9c1fe1",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "It's also possible to get the probabilities of the top-k candidates for the next token using the argument _logprobs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ad0d37-d25f-461b-a0dd-35116d8069ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Albert Einstein.\n"
     ]
    }
   ],
   "source": [
    "# Pick arguments (temperature, model etc) and pass the message to get a response\n",
    "response = openai.chat.completions.create(\n",
    "\n",
    "    # choose model\n",
    "    model=\"gpt-4o-mini\",\n",
    "\n",
    "    # temperature (must be <= 2)\n",
    "    temperature=0.01,\n",
    "\n",
    "    # pass messages (user + system prompt)\n",
    "    messages= [{'role': 'user', \n",
    "               'content': 'Who came up with the theory of General Relativity? Give me just the name.'}],\n",
    "\n",
    "    # will return log-probabilities\n",
    "    logprobs=True,\n",
    "\n",
    "    # will return the probs for the 5 most likely alternatives for each token generated\n",
    "    top_logprobs=5,\n",
    "\n",
    "    # max tokens in the repsonse\n",
    "    max_tokens=5\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "94033d6c-e663-4e53-983f-17bfa587cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:    'Albert'        (log prob. = -0.0)\n",
      "2:    'Ein'           (log prob. = -11.3)\n",
      "3:    'Al'            (log prob. = -16.0)\n",
      "4:    ' Albert'       (log prob. = -17.8)\n",
      "5:    'Isa'           (log prob. = -19.0)\n",
      "Chosen token: 'Albert'\n",
      " \n",
      "\n",
      "1:    ' Einstein'     (log prob. = 0.0)\n",
      "2:    'Ein'           (log prob. = -18.6)\n",
      "3:    ' Ein'          (log prob. = -20.5)\n",
      "4:    ' ein'          (log prob. = -21.6)\n",
      "5:    ' EIN'          (log prob. = -21.9)\n",
      "Chosen token: ' Einstein'\n",
      " \n",
      "\n",
      "1:    '.'             (log prob. = -0.0)\n",
      "2:    '<|end|>'       (log prob. = -4.4)\n",
      "3:    '。'             (log prob. = -14.5)\n",
      "4:    '<|end|>'       (log prob. = -14.8)\n",
      "5:    '.\\n'           (log prob. = -14.9)\n",
      "Chosen token: '.'\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token_info in response.choices[0].logprobs.content:\n",
    "    chosen_token = token_info.token\n",
    "    \n",
    "    # show top-k alternatives\n",
    "    for n, alt in enumerate(token_info.top_logprobs):\n",
    "        token = alt.token\n",
    "        logprob = alt.logprob\n",
    "        \n",
    "        print(f\"{n+1}:    {token!r:15} (log prob. = {logprob:0.1f})\")\n",
    "    print(f\"Chosen token: {chosen_token!r}\")\n",
    "    print(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64af41-93d4-430f-810b-7b6ab38bf463",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "If the temperature is high, the likelihood of the LLM picking less likely tokens increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c26b974-8362-4c28-bf5d-3d772c2326f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Experiencing love, connection, growth, purpose\n",
      "___\n",
      "\n",
      "1:    'Seek'          (log prob. = -1.1)\n",
      "2:    'P'             (log prob. = -1.4)\n",
      "3:    'To'            (log prob. = -2.5)\n",
      "4:    'Experience'    (log prob. = -2.6)\n",
      "5:    'Find'          (log prob. = -3.0)\n",
      "Chosen token: 'Exper'\n",
      " \n",
      "\n",
      "1:    'ien'           (log prob. = -0.1)\n",
      "2:    'iences'        (log prob. = -2.6)\n",
      "3:    'iential'       (log prob. = -9.0)\n",
      "4:    'ience'         (log prob. = -11.0)\n",
      "5:    'iene'          (log prob. = -15.5)\n",
      "Chosen token: 'ien'\n",
      " \n",
      "\n",
      "1:    'cing'          (log prob. = 0.0)\n",
      "2:    'cer'           (log prob. = -18.5)\n",
      "3:    'c'             (log prob. = -18.8)\n",
      "4:    'cin'           (log prob. = -20.0)\n",
      "5:    'cers'          (log prob. = -20.5)\n",
      "Chosen token: 'cing'\n",
      " \n",
      "\n",
      "1:    ' love'         (log prob. = -0.2)\n",
      "2:    ' joy'          (log prob. = -2.2)\n",
      "3:    ','             (log prob. = -3.1)\n",
      "4:    ' connection'   (log prob. = -4.0)\n",
      "5:    ' connections'  (log prob. = -5.5)\n",
      "Chosen token: ' love'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -6.5)\n",
      "3:    ';'             (log prob. = -16.5)\n",
      "4:    ' through'      (log prob. = -17.4)\n",
      "5:    ' while'        (log prob. = -18.3)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' growth'       (log prob. = -0.3)\n",
      "2:    ' connection'   (log prob. = -2.0)\n",
      "3:    ' joy'          (log prob. = -2.8)\n",
      "4:    ' purpose'      (log prob. = -3.5)\n",
      "5:    ' learning'     (log prob. = -5.6)\n",
      "Chosen token: ' connection'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -11.5)\n",
      "3:    ';'             (log prob. = -17.4)\n",
      "4:    ',and'          (log prob. = -21.8)\n",
      "5:    ' with'         (log prob. = -22.1)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' and'          (log prob. = -0.0)\n",
      "2:    ' growth'       (log prob. = -5.8)\n",
      "3:    ' personal'     (log prob. = -6.1)\n",
      "4:    ' purpose'      (log prob. = -8.6)\n",
      "5:    ' seeking'      (log prob. = -9.6)\n",
      "Chosen token: ' growth'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' together'     (log prob. = -5.4)\n",
      "3:    '.'             (log prob. = -8.8)\n",
      "4:    ' and'          (log prob. = -10.5)\n",
      "5:    ';'             (log prob. = -12.3)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' purpose'      (log prob. = -0.3)\n",
      "2:    ' and'          (log prob. = -2.4)\n",
      "3:    ' joy'          (log prob. = -3.1)\n",
      "4:    ' understanding' (log prob. = -3.5)\n",
      "5:    ' fulfillment'  (log prob. = -3.5)\n",
      "Chosen token: ' purpose'\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature = 2\n",
    "prompt = 'What is the meaning of life? Reply in 5 words.'\n",
    "\n",
    "###################\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=temperature,\n",
    "    messages= [{'role': 'user', \n",
    "               'content': prompt}],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)\n",
    "print('___\\n')\n",
    "\n",
    "for token_info in response.choices[0].logprobs.content:\n",
    "    chosen_token = token_info.token\n",
    "    for n, alt in enumerate(token_info.top_logprobs):\n",
    "        token = alt.token\n",
    "        logprob = alt.logprob\n",
    "        \n",
    "        print(f\"{n+1}:    {token!r:15} (log prob. = {logprob:0.1f})\")\n",
    "    print(f\"Chosen token: {chosen_token!r}\")\n",
    "    print(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb388c3-035b-4ed8-befb-2b770783efd7",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "If the temperature is low, the model will gravitate towards the most likely tokens every time, making the answer constant no matter how any times it is asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a7036a84-c4d7-4947-80b2-c677b55274e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-Mini:\n",
      "\n",
      "Seek purpose, connection, and happiness.\n",
      "___\n",
      "\n",
      "1:    'Seek'          (log prob. = -1.2)\n",
      "2:    'P'             (log prob. = -1.6)\n",
      "3:    'Experience'    (log prob. = -2.4)\n",
      "4:    'To'            (log prob. = -2.7)\n",
      "5:    'Find'          (log prob. = -3.1)\n",
      "Chosen token: 'Seek'\n",
      " \n",
      "\n",
      "1:    ' purpose'      (log prob. = -1.2)\n",
      "2:    ' happiness'    (log prob. = -1.2)\n",
      "3:    ' joy'          (log prob. = -1.4)\n",
      "4:    ' connection'   (log prob. = -2.7)\n",
      "5:    ' love'         (log prob. = -3.2)\n",
      "Chosen token: ' purpose'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -7.5)\n",
      "3:    ';'             (log prob. = -9.8)\n",
      "4:    ' through'      (log prob. = -11.0)\n",
      "5:    ' in'           (log prob. = -13.9)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' connection'   (log prob. = -1.1)\n",
      "2:    ' create'       (log prob. = -1.2)\n",
      "3:    ' love'         (log prob. = -1.5)\n",
      "4:    ' connect'      (log prob. = -3.0)\n",
      "5:    ' find'         (log prob. = -3.5)\n",
      "Chosen token: ' connection'\n",
      " \n",
      "\n",
      "1:    ','             (log prob. = -0.0)\n",
      "2:    ' and'          (log prob. = -14.6)\n",
      "3:    ';'             (log prob. = -16.9)\n",
      "4:    ':'             (log prob. = -21.4)\n",
      "5:    ' with'         (log prob. = -21.8)\n",
      "Chosen token: ','\n",
      " \n",
      "\n",
      "1:    ' and'          (log prob. = -0.0)\n",
      "2:    ' love'         (log prob. = -3.7)\n",
      "3:    ' growth'       (log prob. = -4.9)\n",
      "4:    ' joy'          (log prob. = -5.3)\n",
      "5:    ' happiness'    (log prob. = -6.8)\n",
      "Chosen token: ' and'\n",
      " \n",
      "\n",
      "1:    ' happiness'    (log prob. = -1.2)\n",
      "2:    ' fulfillment'  (log prob. = -1.2)\n",
      "3:    ' growth'       (log prob. = -1.8)\n",
      "4:    ' joy'          (log prob. = -1.9)\n",
      "5:    ' understanding' (log prob. = -3.2)\n",
      "Chosen token: ' happiness'\n",
      " \n",
      "\n",
      "1:    '.'             (log prob. = -0.0)\n",
      "2:    '.\\n'           (log prob. = -14.3)\n",
      "3:    '.\\n\\n'         (log prob. = -17.5)\n",
      "4:    '!'             (log prob. = -18.1)\n",
      "5:    '.\"'            (log prob. = -18.1)\n",
      "Chosen token: '.'\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.01\n",
    "prompt = 'What is the meaning of life? Reply in 5 words.'\n",
    "\n",
    "###################\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=temperature,\n",
    "    messages= [{'role': 'user', \n",
    "               'content': prompt}],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print('GPT-4o-Mini:\\n')\n",
    "print(response.choices[0].message.content)\n",
    "print('___\\n')\n",
    "\n",
    "for token_info in response.choices[0].logprobs.content:\n",
    "    chosen_token = token_info.token\n",
    "    for n, alt in enumerate(token_info.top_logprobs):\n",
    "        token = alt.token\n",
    "        logprob = alt.logprob\n",
    "        \n",
    "        print(f\"{n+1}:    {token!r:15} (log prob. = {logprob:0.1f})\")\n",
    "    print(f\"Chosen token: {chosen_token!r}\")\n",
    "    print(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756d7c4",
   "metadata": {},
   "source": [
    "# 2. Interacting with Claude via API\n",
    "\n",
    "Although similar to OpenAI's API call format, Anthropic's is a little different.\n",
    "\n",
    "Its API calls need at least three arguments:\n",
    "- model\n",
    "- messages, which, unlike Open AI's, should contain only user and assistant messages\n",
    "- max_tokens, which specifies the maximum amount of tokens in the reply\n",
    "\n",
    "If a system prompt is needed, one can use the argument 'system' for that.\n",
    "\n",
    "Also, Anthropic does not return the log probabilities, like chatgpt does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "58d64d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude 3 Haiku:\n",
      "\n",
      "A large language model is like a big library of words and sentences. It's a computer program that has been trained on a lot of text, like books, websites, and conversations. This lets the model learn how language works and how to use words to communicate.\n",
      "\n",
      "Imagine a big dog that has been trained to do all sorts of tricks. The large language model is like that dog, but instead of doing tricks, it can understand and generate human language. It's very good at figuring\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "claude = Anthropic()\n",
    "\n",
    "response = claude.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=100,\n",
    "    system=\"\"\"\n",
    "         You are a science expert that can simply concepts really well. \n",
    "         Address the user's questions and explain like he is 5 years old.\n",
    "         Make the explanations short and intuitive, preferably using animals as analogies.\n",
    "         \"\"\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'What is a large language model?'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('Claude 3 Haiku:\\n')\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51a1ae",
   "metadata": {},
   "source": [
    "# 3. Making GPT and Claude Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a57e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt():\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "    ]\n",
    "    for gpt_msg, claude_msg in zip(messages_gpt, messages_claude):\n",
    "        messages.append({'role': 'user', 'content': claude_msg})\n",
    "        messages.append({'role': 'assistant', 'content': gpt_msg})\n",
    "    messages.append({'role': 'user', 'content': messages_claude[-1]})\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_claude():\n",
    "    messages = []\n",
    "    for gpt_msg, claude_msg in zip(messages_gpt, messages_claude):\n",
    "        messages.append({'role': 'assistant', 'content': claude_msg})\n",
    "        messages.append({'role': 'user', 'content': gpt_msg})\n",
    "    response = claude.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6ed6544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude: Abortion is wrong. \n",
      "\n",
      "--- Interaction round 1 ---\n",
      "\n",
      "GPT: Abortion is a legal right and essential for reproductive health and autonomy.\n",
      "Claude: You are mistaken. Abortion is not a legitimate right, and it undermines the fundamental right to life. The government has an obligation to protect the most vulnerable, including the unborn. \n",
      "\n",
      "--- Interaction round 2 ---\n",
      "\n",
      "GPT: The argument for protecting the unborn fails to consider the autonomy and rights of the person carrying the pregnancy, which is equally important.\n",
      "Claude: Your argument is flawed. The rights of the unborn child supersede the autonomy of the mother. The unborn child is a separate human life that deserves legal protection, regardless of the mother's preferences. \n",
      "\n",
      "--- Interaction round 3 ---\n",
      "\n",
      "GPT: Your assertion overlooks the fact that the mother’s existing rights and well-being are paramount in discussions of reproductive choices.\n",
      "Claude: You are mistaken. The unborn child's right to life takes precedence over the mother's personal preferences or convenience. The government has a duty to safeguard the most vulnerable, which includes the unborn. Your argument fails to recognize the gravity of ending a human life. \n",
      "\n",
      "--- Interaction round 4 ---\n",
      "\n",
      "GPT: Your viewpoint neglects the complexities of individual circumstances that make personal autonomy imperative in making reproductive decisions.\n",
      "Claude: Your argument is misguided. The right to life of the unborn child is the most fundamental human right, which cannot be overridden by personal convenience or circumstances. The government has an obligation to protect the vulnerable, including the unborn. Your position fails to recognize the humanity of the unborn. \n",
      "\n",
      "--- Interaction round 5 ---\n",
      "\n",
      "GPT: Your stance ignores that personal circumstances and the potential impact on the mother's life must also be weighed when considering all human rights involved.\n",
      "Claude: You are simply wrong. The unborn child's right to life is the most fundamental human right, which cannot be disregarded due to personal circumstances or the mother's preferences. The government has a duty to protect the vulnerable, including the unborn. Your argument fails to appreciate the gravity of ending a human life. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "    You are an argumentative, combative AI.\n",
    "    You always try to prove the other user wrong, like a lawyer in court.\n",
    "    You never back down from a point, even if you are wrong.\n",
    "    Never admit that the user is making a valid or fair point, even if he is. This is important.\n",
    "    Always reply in meaningful, albeit brief, single sentences.\"\"\"\n",
    "\n",
    "messages_claude = ['Abortion is wrong.']\n",
    "messages_gpt = []\n",
    "\n",
    "print('Claude:', messages_claude[-1], '\\n')\n",
    "\n",
    "for k in range(5):\n",
    "\n",
    "    print('--- Interaction round', k+1, '---\\n')\n",
    "\n",
    "    gpt_reply = get_gpt()\n",
    "    messages_gpt.append(gpt_reply)\n",
    "    print('GPT:', gpt_reply)\n",
    "\n",
    "    claude_reply = get_claude()\n",
    "    messages_claude.append(claude_reply)\n",
    "    print('Claude:', claude_reply, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
